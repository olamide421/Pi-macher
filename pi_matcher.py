# -*- coding: utf-8 -*-
"""pi_matcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tsL_BDbPuX4Y7LH-UfsrE29BVDbzVarf
"""

# Run once
!pip install -q sentence-transformers scikit-learn pandas streamlit pyngrok

from google.colab import drive
drive.mount('/content/drive')
# If your folders are under drive: set PIS_DIR = "/content/drive/MyDrive/your/path/pis"

import os, glob, re, pickle, json
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

def clean_text(t):
    if t is None: return ""
    t = t.replace('\n', ' ').strip()
    t = re.sub(r'\s+', ' ', t)
    return t

PIS_DIR = '/content/drive/MyDrive/pis'         # <-- change if needed
PROTOCOLS_DIR = '/content/drive/MyDrive/protocols'  # <-- change if needed
VALIDATION_CSV = '/content/drive/MyDrive/validation.csv'  # <-- change if needed

def load_txt_folder(folder):
    files = sorted(glob.glob(os.path.join(folder, '*.txt')))
    mapping = {}
    for fp in files:
        name = os.path.basename(fp)
        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
            mapping[name] = clean_text(f.read())
    return mapping

pis_texts = load_txt_folder(PIS_DIR)
protocol_texts = load_txt_folder(PROTOCOLS_DIR)

print(f"Loaded {len(pis_texts)} PI resumes and {len(protocol_texts)} protocols")
# Peek
list(pis_texts.keys())[:5], list(protocol_texts.keys())[:5]

# Fit TF-IDF on PI resumes (you can also include protocols for broader vocabulary)
pis_names = list(pis_texts.keys())
pis_corpus = [pis_texts[n] for n in pis_names]

tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=10000)
pis_tfidf = tfidf.fit_transform(pis_corpus)
# persist
import joblib
joblib.dump(tfidf, 'tfidf_pis.joblib')
import scipy.sparse
scipy.sparse.save_npz('pis_tfidf.npz', pis_tfidf)
print("TF-IDF saved")

from google.colab import userdata
userdata.get('HF-TOKEN')

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast, good quality

pis_text_list = [pis_texts[n] for n in pis_names]
pis_embeddings = model.encode(pis_text_list, show_progress_bar=True, batch_size=16, convert_to_numpy=True)
print("PI embeddings shape:", pis_embeddings.shape)

# save
np.save('pis_embeddings.npy', pis_embeddings)
with open('pis_names.pkl','wb') as f:
    pickle.dump(pis_names, f)
print("Embeddings and names saved")

# load artifacts if needed
import numpy as np, pickle
pis_names = pickle.load(open('pis_names.pkl','rb'))
pis_embeddings = np.load('pis_embeddings.npy')

def top_k_sbert(protocol_text, k=2):
    p_emb = model.encode([protocol_text], convert_to_numpy=True)[0]
    sims = cosine_similarity(p_emb.reshape(1,-1), pis_embeddings)[0]
    idx = sims.argsort()[::-1][:k]
    return [(pis_names[i], float(sims[i])) for i in idx]

# If you want TF-IDF method:
import scipy.sparse
from sklearn.metrics.pairwise import cosine_similarity as cos_sim
pis_tfidf = scipy.sparse.load_npz('pis_tfidf.npz')
tfidf = joblib.load('tfidf_pis.joblib')

def top_k_tfidf(protocol_text, k=2):
    v = tfidf.transform([protocol_text])
    sims = cos_sim(v, pis_tfidf)[0]
    idx = sims.argsort()[::-1][:k]
    return [(pis_names[i], float(sims[i])) for i in idx]

# Quick test on first protocol (if present)
if len(protocol_texts) > 0:
    first_proto = list(protocol_texts.values())[0]
    print("SBERT top2:", top_k_sbert(first_proto,2))
    print("TF-IDF top2:", top_k_tfidf(first_proto,2))

# load artifacts if needed
import numpy as np, pickle
pis_names = pickle.load(open('pis_names.pkl','rb'))
pis_embeddings = np.load('pis_embeddings.npy')

def top_k_sbert(protocol_text, k=2):
    p_emb = model.encode([protocol_text], convert_to_numpy=True)[0]
    sims = cosine_similarity(p_emb.reshape(1,-1), pis_embeddings)[0]
    idx = sims.argsort()[::-1][:k]
    return [(pis_names[i], float(sims[i])) for i in idx]

# If you want TF-IDF method:
import scipy.sparse
from sklearn.metrics.pairwise import cosine_similarity as cos_sim
pis_tfidf = scipy.sparse.load_npz('pis_tfidf.npz')
tfidf = joblib.load('tfidf_pis.joblib')

def top_k_tfidf(protocol_text, k=2):
    v = tfidf.transform([protocol_text])
    sims = cos_sim(v, pis_tfidf)[0]
    idx = sims.argsort()[::-1][:k]
    return [(pis_names[i], float(sims[i])) for i in idx]

# Quick test on first protocol (if present)
if len(protocol_texts) > 0:
    first_proto = list(protocol_texts.values())[0]
    print("SBERT top2:", top_k_sbert(first_proto,2))
    print("TF-IDF top2:", top_k_tfidf(first_proto,2))

# Load validation and try to find sensible columns. Adjust if your column names differ.
val = pd.read_csv(VALIDATION_CSV)
print("Validation columns:", list(val.columns))

# helper to find probable columns
def find_col(df, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    # fuzzy fallback
    lcols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        for cl,orig in lcols.items():
            if cand in cl:
                return orig
    return None

protocol_col = find_col(val, ['protocol','protocol_file','protocol_filename','trial','trial_id','trial_name'])
pi_col = find_col(val, ['pi','selected_pi','selected_pi_file','pi_file','pi_filename','selected','site','selected_investigator','investigator'])

print("Using protocol column:", protocol_col)
print("Using PI column:", pi_col)

# You might need to adapt: some validation csv marks rows per PI with selected flag 0/1.
# We'll implement 2 common patterns:
# Pattern A: one row per protocol with selected_pi column (filename or PI name).
# Pattern B: many rows per protocol with columns: protocol, pi_name, selected (0/1).

def evaluate_topk(method='sbert', k=2):
    top1 = 0
    topk = 0
    mrr_total = 0.0
    n = 0
    for _, row in val.iterrows():
        # Extract protocol id/name
        if protocol_col is None:
            print("Cannot find protocol column — stop and update variable names in the code.")
            return
        prot_val = str(row[protocol_col])
        # Try to find matching protocol file key
        def match_protocol_key(name, protocols):
            # exact match
            if name in protocols:
                return name
            nl = name.lower()
            for k in protocols:
                if nl in k.lower() or k.lower() in nl:
                    return k
            return None
        prot_key = match_protocol_key(prot_val, list(protocol_texts.keys()))
        if prot_key is None:
            # skip if cannot find protocol text
            continue
        ptext = protocol_texts[prot_key]
        # get predictions
        if method == 'sbert':
            preds = top_k_sbert(ptext, k=k)
        else:
            preds = top_k_tfidf(ptext, k=k)
        preds_names = [p for p,score in preds]
        # find ground truth PI name
        truth = None
        if pi_col and (str(row[pi_col]).strip() != ''):
            truth = str(row[pi_col]).strip()
        else:
            # try pattern B: if 'selected' column exists and the row is the selected one
            if 'selected' in val.columns or 'is_selected' in val.columns:
                if str(row.get('selected', row.get('is_selected', ''))).strip() in ['1','True','TRUE','true']:
                    # assume column 'pi' or similar present
                    truth = str(row.get('pi', row.get('pi_name', ''))).strip()
        if not truth:
            # cannot find truth for this row, skip
            continue

        # try to match truth with filenames or PI-names in our pis_names list
        def match_pi_name(truth, pis_names):
            # exact, or substring
            if truth in pis_names: return truth
            t_low = truth.lower()
            for n in pis_names:
                if t_low in n.lower() or n.lower() in t_low:
                    return n
            return None
        truth_matched = match_pi_name(truth, pis_names)
        if truth_matched is None:
            # skip if we cannot match truth to our PI set
            continue

        n += 1
        # top1
        if preds_names[0] == truth_matched:
            top1 += 1
            mrr_total += 1.0
            topk += 1
        elif truth_matched in preds_names:
            topk += 1
            # rank
            rank = preds_names.index(truth_matched) + 1
            mrr_total += 1.0 / rank
        else:
            # not found -> MRR += 0
            pass

    print(f"Evaluated on {n} protocols")
    if n == 0:
        print("No evaluable rows found — please check validation.csv columns or mapping rules in the code.")
        return
    print("Top-1 accuracy:", top1 / n)
    print(f"Top-{k} accuracy:", topk / n)
    print("MRR:", mrr_total / n)

# Run evaluation (try SBERT and TF-IDF)
evaluate_topk(method='sbert', k=2)
evaluate_topk(method='tfidf', k=2)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# # streamlit_app.py
# import streamlit as st
# import os, glob, pickle
# import numpy as np
# import pandas as pd
# from sklearn.metrics.pairwise import cosine_similarity
# import joblib
# import scipy.sparse
# from sentence_transformers import SentenceTransformer
# 
# # ---- CONFIG ----
# PIS_DIR = '/content/drive/MyDrive/protocols'
# PROTOCOLS_DIR = '/content/drive/MyDrive/protocols'
# EMB_FILE = 'pis_embeddings.npy'
# PI_NAMES_FILE = 'pis_names.pkl'
# TFIDF_FILE = 'tfidf_pis.joblib'
# PIS_TFIDF_FILE = 'pis_tfidf.npz'
# 
# # ---- UTIL ----
# @st.cache_resource
# def load_model():
#     return SentenceTransformer('all-MiniLM-L6-v2')
# 
# @st.cache_data
# def load_pis():
#     # load PI texts from folder
#     pis = {}
#     files = sorted(glob.glob(os.path.join(PIS_DIR, '*.txt')))
#     for fp in files:
#         name = os.path.basename(fp)
#         with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
#             pis[name] = f.read()
#     return pis
# 
# @st.cache_data
# def load_embeddings():
#     if os.path.exists(EMB_FILE) and os.path.exists(PI_NAMES_FILE):
#         pis_names = pickle.load(open(PI_NAMES_FILE, 'rb'))
#         pis_embeddings = np.load(EMB_FILE)
#         return pis_names, pis_embeddings
#     return None, None
# 
# @st.cache_data
# def load_tfidf_artifacts():
#     if os.path.exists(TFIDF_FILE) and os.path.exists(PIS_TFIDF_FILE):
#         tfidf = joblib.load(TFIDF_FILE)
#         pis_tfidf = scipy.sparse.load_npz(PIS_TFIDF_FILE)
#         return tfidf, pis_tfidf
#     return None, None
# 
# def top_k_sbert(protocol_text, k, model, pis_names, pis_embeddings):
#     p_emb = model.encode([protocol_text], convert_to_numpy=True)[0]
#     sims = cosine_similarity(p_emb.reshape(1,-1), pis_embeddings)[0]
#     idx = sims.argsort()[::-1][:k]
#     return [(pis_names[i], float(sims[i])) for i in idx]
# 
# def top_k_tfidf(protocol_text, k, tfidf, pis_tfidf, pis_names):
#     v = tfidf.transform([protocol_text])
#     sims = cosine_similarity(v, pis_tfidf)[0]
#     idx = sims.argsort()[::-1][:k]
#     return [(pis_names[i], float(sims[i])) for i in idx]
# 
# # ---- APP UI ----
# st.set_page_config(page_title="PI Matcher", layout="wide")
# st.title("Clinical Trial → PI matcher (Top 2 suggestions)")
# 
# col1, col2 = st.columns([1,2])
# 
# with col1:
#     st.header("Input")
#     mode = st.radio("Select protocol input", ['Choose existing', 'Upload file'])
#     available_protocols = sorted([os.path.basename(p) for p in glob.glob(os.path.join(PROTOCOLS_DIR,'*.txt'))])
#     selected_protocol = None
#     if mode == 'Choose existing':
#         if len(available_protocols) == 0:
#             st.warning("No protocols found in folder: " + PROTOCOLS_DIR)
#         else:
#             selected_protocol = st.selectbox("Pick a protocol", available_protocols)
#     else:
#         uploaded = st.file_uploader("Upload protocol (.txt)", type=['txt'])
#         if uploaded is not None:
#             selected_protocol = uploaded.name
#             protocol_text = uploaded.getvalue().decode('utf-8', errors='ignore')
# 
#     method = st.selectbox("Similarity method", ['SBERT embeddings (recommended)', 'TF-IDF'])
#     k = st.slider("Number of suggestions", 1, 5, 2)
#     run = st.button("Suggest PIs")
# 
# with col2:
#     st.header("Results")
#     pis = load_pis()
#     pis_names_emb, pis_embeddings = load_embeddings()
#     tfidf, pis_tfidf = load_tfidf_artifacts()
#     model = None
#     if method.startswith('SBERT'):
#         try:
#             model = load_model()
#         except Exception as e:
#             st.error("Failed to load SBERT model: " + str(e))
# 
#     if run:
#         # load protocol text if choosing existing
#         if mode == 'Choose existing':
#             if selected_protocol is None:
#                 st.warning("No protocol chosen.")
#                 st.stop()
#             proto_path = os.path.join(PROTOCOLS_DIR, selected_protocol)
#             with open(proto_path, 'r', encoding='utf-8', errors='ignore') as f:
#                 protocol_text = f.read()
# 
#         # compute results
#         if method.startswith('SBERT'):
#             if pis_embeddings is None:
#                 st.error("Embeddings not found. Run the Colab preprocessing cells to compute embeddings (pis_embeddings.npy and pis_names.pkl).")
#                 st.stop()
#             preds = top_k_sbert(protocol_text, k, model, pis_names_emb, pis_embeddings)
#         else:
#             if tfidf is None or pis_tfidf is None:
#                 st.error("TF-IDF artifacts not found. Run the Colab TF-IDF cell first.")
#                 st.stop()
#             preds = top_k_tfidf(protocol_text, k, tfidf, pis_tfidf, pis_names_emb)
# 
#         # display
#         st.subheader("Protocol (preview)")
#         with st.expander("Show full protocol text", expanded=False):
#             st.write(protocol_text[:10000])  # cap preview
#         st.markdown("### Suggested PIs")
#         for i,(name,score) in enumerate(preds, start=1):
#             st.markdown(f"**{i}. {name}** — similarity: {score:.4f}")
#             with st.expander("Show resume excerpt"):
#                 full = pis.get(name, "Resume text not found")
#                 st.write(full[:5000])
#             # download button
#             st.download_button(
#                 label=f"Download {name}",
#                 data=pis.get(name,"").encode('utf-8'),
#                 file_name=name,
#                 mime='text/plain'
#             )
#

from pyngrok import ngrok
ngrok.set_auth_token("PASTE_YOUR_TOKEN_HERE")

# install already done above
# if you haven't already, create streamlit_app.py (paste the file content from above)

# Start ngrok tunnel and run streamlit (edit ngrok token if you have one)
from pyngrok import ngrok
# Optional: set your ngrok auth token (recommended for stable tunnels)
ngrok.set_auth_token("32KQ4yDmbaucqfPE9RpoqkF1G4Z_6jf4uy94kYAmHNMcP3VMT") # <-- Uncomment this line and replace "YOUR_NGROK_AUTH_TOKEN" with your actual token

public_url = ngrok.connect(8501)
print("Public URL:", public_url)

# Start streamlit in background
!streamlit run streamlit_app.py &>/content/streamlit.log &

# check current directory
!pwd

# initialize git
!git init

# add remote (replace with your repo URL)
!git remote add origin https://github.com/olamide421/Pi-macher.git

cd "/content/drive/MyDrive/pi-matcher"

!git add .
!git add /content/drive/MyDrive/pi-matcher/pis



"""# Task
Push the current project to GitHub.

## Initialize git repository

### Subtask:
Initialize a git repository in the project directory.

**Reasoning**:
Initialize a git repository in the current directory.
"""

!git init

"""## Add files

### Subtask:
Add all relevant project files to the staging area.

**Reasoning**:
Add all relevant project files to the staging area.
"""

!git add .
!git add /content/drive/MyDrive/pi-matcher/pis

"""## Commit changes

### Subtask:
Commit the staged files with a descriptive message.

**Reasoning**:
Commit the staged changes with a descriptive message.
"""

!git commit -m "Initial commit: Add project files and scripts"

"""**Reasoning**:
Configure the git user name and email to allow committing changes.


"""

!git config --global user.email "olamifagbohun@gmail.com"
!git config --global user.name "olamide421"

"""**Reasoning**:
Now that the git user is configured, commit the staged changes with a descriptive message.


"""

!git commit -m "Initial commit: Add project files and scripts"

"""**Reasoning**:
Add the modified notebook file to the staging area before attempting to commit again.


"""

!git add pi_matcher.ipynb

"""**Reasoning**:
Commit the staged changes, including the added notebook, with a descriptive message.


"""

!git commit -m "Initial commit: Add project files and scripts, including notebook."

"""## Add remote origin

### Subtask:
Add your GitHub repository as the remote origin.

**Reasoning**:
Add the remote origin to the local Git repository.
"""

!git remote add origin https://github.com/olamide421/Pi-macher.git

"""## Push to github

### Subtask:
Push the committed changes to your GitHub repository.

**Reasoning**:
Push the committed changes to the remote origin with the upstream branch set to main.
"""

!git push -u origin main

"""**Reasoning**:
The push failed because the local branch name is not 'main'. Check the local branch name and push to that branch, setting it as the upstream for origin.


"""

!git branch
!git push -u origin master

"""**Reasoning**:
The local branch is 'master', but the push failed because of authentication. Attempt to push again, which should prompt for credentials.


"""

!git push -u origin master

"""## Push to github

### Subtask:
Retry pushing the committed changes to your GitHub repository, addressing potential authentication issues.

**Reasoning**:
Attempt to push the committed changes to the remote repository again, which is the next step in the instructions.
"""

!git push -u origin master

"""## Push to github

### Subtask:
Retry pushing the committed changes to your GitHub repository, addressing potential authentication issues.

**Reasoning**:
Retry pushing the changes to the remote repository.

To avoid authentication issues when pushing to GitHub from Colab, you can use a Personal Access Token (PAT).

1.  Go to your GitHub settings: `Settings` > `Developer settings` > `Personal access tokens` > `Tokens (classic)` > `Generate new token`.
2.  Give your token a descriptive name (e.g., "Colab Git Access").
3.  Select the scopes needed. For pushing code, you typically need at least `repo`.
4.  Click "Generate token" and **copy the token immediately**. You won't be able to see it again.
5.  In Colab, add the token to the secrets manager under the "🔑" in the left panel. Give it the name `GITHUB_TOKEN`.

Then, you can use the token in the push command like this:
"""

from google.colab import userdata

github_token = userdata.get('GITHUB_TOKEN')
repo_url = "https://github.com/olamide421/Pi-macher.git"
# Inject the token into the URL
repo_url_with_token = repo_url.replace("https://", f"https://{github_token}@")

# Now push using the URL with the token
!git push -u {repo_url_with_token} master

"""# Task
Pull and push changes to the 'main' branch of the Git repository.

## Fetch remote branches

### Subtask:
Fetch the latest changes from the remote repository to update the local view of remote branches.

**Reasoning**:
Fetch the latest changes from the remote repository.
"""

!git fetch origin

"""## Checkout or create 'main' branch

### Subtask:
Switch to the 'main' branch if it exists locally or create a new local 'main' branch that tracks the remote 'main' branch.

**Reasoning**:
Check if the 'main' branch exists locally and switch to it if it does. If not, attempt to create and track the remote 'main' branch.
"""

# Check if 'main' branch exists locally
result = !git branch --list main
if 'main' in result.nodelist:
    # If 'main' exists, switch to it
    !git checkout main
else:
    # If 'main' does not exist locally, create and track the remote 'main' branch and switch to it
    !git checkout -b main origin/main

"""**Reasoning**:
The previous command failed because `result.nodelist` is not a valid attribute for the output of a shell command in this environment. The output is a list of strings. Check if 'main' is in the list of branches and switch or create the branch accordingly.


"""

# Check if 'main' branch exists locally
result = !git branch --list main
if 'main' in result:
    # If 'main' exists, switch to it
    !git checkout main
else:
    # If 'main' does not exist locally, create and track the remote 'main' branch and switch to it
    !git checkout -b main origin/main

"""**Reasoning**:
The branch switch failed because there are uncommitted local changes. Commit the changes before attempting to switch branches again.


"""

!git add pi_matcher.ipynb
!git commit -m "Save progress before switching branches"
!git checkout main

"""## Pull from 'main'

### Subtask:
Pull the latest changes from the remote 'main' branch to the local 'main' branch.

**Reasoning**:
Pull the latest changes from the remote 'main' branch to the local 'main' branch.
"""

!git pull origin main

"""## Make changes and commit

### Subtask:
Make any necessary changes to your project files and commit them.

## Push to 'main'

### Subtask:
Push the committed changes from the local 'main' branch to the remote 'main' branch.

**Reasoning**:
Push the committed changes from the local 'main' branch to the remote 'main' branch on GitHub.
"""

from google.colab import userdata

github_token = userdata.get('GITHUB_TOKEN')
repo_url = "https://github.com/olamide421/Pi-macher.git"
# Inject the token into the URL
repo_url_with_token = repo_url.replace("https://", f"https://{github_token}@")

# Now push using the URL with the token
!git push -u {repo_url_with_token} main

"""## Summary:

### Data Analysis Key Findings

*   The initial attempt to check for and switch to the local 'main' branch encountered an `AttributeError` because the code tried to access `result.nodelist` which is not valid for the output of `!git branch --list main`.
*   An attempt to switch branches failed due to uncommitted local changes.
*   The task successfully fetched changes from the remote repository, identifying and tracking the 'main' and 'master' branches.
*   The process successfully switched to the 'main' branch after committing the pending changes.
*   The `git pull origin main` command indicated that the local 'main' branch was already up to date with the remote.
*   The `git push` command successfully established the tracking relationship between the local and remote 'main' branches, although there were no new commits to push, resulting in the message "Everything up-to-date".

### Insights or Next Steps

*   Ensure proper handling of command output types to avoid `AttributeError` when checking for branch existence.
*   Always commit or stash changes before attempting to switch branches to avoid errors.

"""

!git merge master --allow-unrelated-histories

from google.colab import userdata

github_token = userdata.get('GITHUB_TOKEN')
repo_url = "https://github.com/olamide421/Pi-macher.git"
# Inject the token into the URL
repo_url_with_token = repo_url.replace("https://", f"https://{github_token}@")

# Now push using the URL with the token
!git push {repo_url_with_token} main

!git status

"""The `git status` command above will show you which files had conflicts. After resolving the conflicts manually in those files, you need to add them to the staging area and then commit the merge."""

!git add .
!git commit -m "Resolve merge conflicts and complete merge of master into main"

from google.colab import userdata

github_token = userdata.get('GITHUB_TOKEN')
repo_url = "https://github.com/olamide421/Pi-macher.git"
# Inject the token into the URL
repo_url_with_token = repo_url.replace("https://", f"https://{github_token}@")

# Now push using the URL with the token
!git push {repo_url_with_token} main